<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite App</title>
    <script type="module" crossorigin src="assets/index.af4f6367.js"></script>
    <link rel="stylesheet" href="assets/index.72aacf91.css">
  </head>
  <body>
    <div id="skills-div" class="basic-bar-chart">
      <canvas id="skills"></canvas>
    </div>
    <table id="skills-legend">
      <tr>
        <th style="width: 2rem"></th>
        <th>Completion Level</th>
      </tr>
      <tr>
        <td style="background: #1E293B; border: 3px solid #1C1917"></td>
        <td>Totally Correct</td>
      </tr>
      <tr>
        <td style="background: #475569; border: 3px solid #1C1917"></td>
        <td>Partially Correct</td>
      </tr>
      <tr>
        <td style="background: #CBD5E1; border: 3px solid #1C1917"></td>
        <td>Attempted</td>
      </tr>
      <tr>
        <td style="background: #F1F5F9; border: 3px solid #1C1917"></td>
        <td>Not Attempted</td>
      </tr>
    </table>


    <div id="question-tables-and-doughnuts"></div>

    <div id="cumulative-skill-totals-div" class="basic-bar-chart">
      <canvas id="cumulative-skill-totals"></canvas>
    </div>


    <div class="findings">
      <p class="key-find"><span id="score-seventy-five"></span>% of applications hit 75% or more of skills</li></p>
      <p class="key-find"><span id="score-fifty"></span>% of applications hit 50% or more of skills</li></p>

      <div id='year-over-year' class="question-data">
        <h2>Year Over Year</h2>

        <table id="compare-table" class="basic-table">
          <tr>
            <th style="background: #22C55E">Skill Set</th>
            <th style="background: #22C55E">Ids</th>
            <th style="background: #22C55E">Spring 2023</th>
            <th style="background: #22C55E">Spring 2022</th>
          </tr>
          <tr>
            <td>Flow Control</td>
            <td>gr4, gr5, gr6</td>
            <td>85%</td>
            <td>87%</td>
          </tr>
          <tr>
            <td>For Loops</td>
            <td>hs1, hs2, hs3, hs4, hs5</td>
            <td>89%</td>
            <td>33%</td>
          </tr>
        </table>

      <div class="findings">
        <p>
          While not exactly a fair comparison (each year was judged with different metrics), it's clear that we mostly maintained Flow Control skills, but had a dramatic increase of applicants who got to Loops. So it looks like functions, flow control, and loops is <i>not</i> too much for students to manage.
        </p>
        <br>
      </div>
      </div>

      <h2>Score Notes</h2>
      <p>
        I think we can feel pretty confident that we didn't do damage to our applicants by switching away from Ruby. The scoring system needs refinement. The AirTable scores are too skewed (completions earn way too many points and stack to highly)
      </p>
      <p>
        So, here I'm not trying to weight any skills, I'm just counting whether or not they met a skill. And for completions, it's the same across all questions, even though the later ones are harder, so a "totally correct" betterCheer() is definitely harder than a correct greet().
      </p>
      <p>
        In the second round I'm going to swap some of them to general points instead of by question. Reason being, some skills were inconsistent across questions. For instance, in the `greet` question the applicant would <i>not</i> use interpolation correctly, but then later on they would. So, which "question" earns the point? If they used a skill correctly more than incorrectly, I gave them the point, but we can clean this up as we move forward and just have "question specific skills" and "application wide skills"
      </p>
      <p>
        I have also removed 5 "bad" attempts from the scoring. Sine we do no code screening prior to handing this out, I don't want our data to skew too negative. A "bad" attempt is one where there was clearly no effort put in, think just a bunch of logs that print "I tried" (we get at least one every year). In the future, we should do a minor screen to make sure those who send in a code challenge will commit to at least a week of learning and actual attempts at answering the question.
      </p>
    </div>
    <h2>Problems and Learnings</h2>
    <div class="findings">
      <p>In no particular order:</p>
      <p>
        The for loop is too easy to mislead with. Unlike an `if/else`, a student can perfectly cut and copy a for loop and get points. To see if a student actually understands loops, I think we should just ask them to write one from scratch in the interviews. It's just not possible to test otherwise.
      </p>
      <p>
        I need to provide multiple examples for every question. I foolishly included only 1 example for the last three questions, which led to students hard coding `4` for hideAndSeek, `WIN` for cheer and `VICTORY` for better cheer. When they did hardcode things, I gave them partial credit. This wasn't universal, but it was enough where we can't just blame the students
      </p>
      <p>
        I need to remove "at least three" from hide and seek. Some students took that as a sort of validation challenge, but that check scaled up the difficulty for the problem and I don't want that. Also I literally put in a typo in the arg description where it says "at least 2", not 3. Don't know how I missed that typo.
      </p>
      <p>
        We need to make a video showing students <i>exactly</i> how to format and submit their application. We had examples not wrapped in functions, and multiple replits. Again, there were only a handful, but in order to mark against these faulty attempts, we need to do a better job explaining it for the newer students. Given that 28/33 aced the arrow functions (and +3 used old school functions), we now know pretty much everyone gets it. I wasn't sure prior to this, but now I have the data to back it up. That means giving them a replit to copy with the functions pre-written will help us much more than it will hurt them.
      </p>
      <p>
        I have to make it clearer we actively <i>do not want</i> students to concatenate. I'm going to re-word the concatenate section and remove the link to how to do it from the lessons.
      </p>
      <p>
        I want to add some different skills so that we can have options for "non-wrong" answers. Like, one student figured out that for simple if/else statements a hash works better. I gave them the points for if/else but that's a really strong move and it's lost in the data. Same idea when someone used regex and .match() instead of includes(). This means that a "perfect" score will no longer possible (since students can either-or certain skills), but it will still be clear if students get the main points.
      </p>
      <p>
        We had great completion rates for "getRating", but it's clunky and I don't like it. It only uses 1 OR and no AND statements. If we have time, lets change it before the second round.
      </p>
      <p>
        Most students at least attempted the 4 main problems, and that's great, that means they really weren't scared off by seeing all the questions.
      </p>
    </div>
    <a href="./site-stats.html">Check out the stats for the site itself</a>
  </body>
</html>
